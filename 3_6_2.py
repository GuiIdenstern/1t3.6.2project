# -*- coding: utf-8 -*-
"""3.6.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Ifh7B3ij2BGbzDBtJ4xQ4LNJuUZF45h
"""

# pip install pyspark
# pip install install-jdk
# pip install findspark
from datetime import datetime, date, timedelta
import random

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType
from pyspark.sql.functions import sum, max, min, avg, count, mean, col

spark = SparkSession.builder \
    .appName("example") \
    .getOrCreate()


#Список товаров
product_names=["Printer","Laptop","PC","Smartphone","TV"]

#Количество строк
rows_count=1000

product_schema = StructType([
    StructField("UserId", IntegerType(), False),
    StructField("Date", DateType(), True),
    StructField("ProductName", StringType(), True),
    StructField("Quantity", IntegerType(), True),
    StructField("Price", DoubleType(), True)
])


products=[]
for i in range (rows_count):
  products.append((
      random.randint(1000000,9999999), #UserId
      datetime(datetime.today().year,1,1,0,0,0)+timedelta(days=random.randint(1,364)), #Date
      product_names[random.randint(0,len(product_names)-1)],#Name
      random.randint(0,100),#Quantity
      round(random.randint(1,1000)*random.random(),2)#Price
  ))

products_df=spark.createDataFrame(products,product_schema)
products_df.coalesce(1).write.format("csv").mode('overwrite').save("output/")